from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import UnstructuredFileLoader
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st
import os

st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“ƒ",
)


# Streamingì„ ë‹¤ë£¨ëŠ” í´ë˜ìŠ¤ ìƒì„±
class ChatCallbackHandler(BaseCallbackHandler):
    # ì´ˆê¸°í™” ë©”ì†Œë“œ: *argëŠ” argument(1,2,3,4), **kwargëŠ” keyword argument(a=1, b=2)
    def __init__(self, *args, **kwargs):
        self.message = ""

    # llmì‹œì‘
    def on_llm_start(self, *args, **kwargs):
        # message_box ì´ˆê¸°í™”
        self.message_box = st.empty()

    # llmì¢…ë£Œ
    def on_llm_end(self, *args, **kwargs):
        # messageë¥¼ ì €ì¥
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        # ê¸°ì¡´ ë©”ì‹œì§€ì— ê³„ì† í† í°ì„ ë¶™ì—¬ë‚˜ê°
        self.message += token
        # ì¶”ê°€ë˜ëŠ” ë©”ì‹œì§€ë¥¼ ë©”ì‹œì§€ ë°•ìŠ¤ì— ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ í‘œì‹œ(ì—…ë°ì´íŠ¸)
        self.message_box.markdown(self.message)


llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    # callbacksë¥¼ ë¶€ë¥¼ ë•ŒëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ì œê³µ
    callbacks=[ChatCallbackHandler()],
)


# Embedding Method
# í•¨ìˆ˜ ìœ„ì— ì¶”ê°€í•˜ëŠ” ê²ƒì„ ë°ì½”ë ˆì´í„°ë¼ê³  í•¨
# st.cache_resourceëŠ” ë˜‘ê°™ì€ íŒŒì¼ì´ ë“¤ì–´ì˜¤ëŠ” ê²½ìš° embed_file methodë¥¼ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬í•˜ê²Œ ë˜ì–´ íš¨ìœ¨ì„±ê³¼ ì†ë„, ë¹„ìš© ëª¨ë‘ ì†í•´ë³´ê¸° ë•Œë¬¸ì— ì¬ì‹¤í–‰ë˜ëŠ” ê²ƒì„ ë§‰ì•„ì¤Œ
@st.cache_resource(show_spinner="Embedding file...")
def embed_file(file):
    # file ì½ê¸°
    file_content = file.read()
    # st.write(file_content)
    # file path ê²°ì •
    file_path = f"./.cache/files/{file.name}"
    directory = os.path.dirname(file_path)

    if not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)
        print(f"Directory created: {directory}")

    try:
        # st.write(file_path)
        # file_pathì— w(rite)b(inary) ëª¨ë“œë¡œ ì—´ê¸°
        with open(file_path, "wb") as f:
            # file ë‚´ìš©ì„ ê¸°ë¡
            f.write(file_content)
    except Exception as e:
        print(f"An error occurred: {e}")

    # cache directory ì„¤ì •: ë¡œì»¬ì— íŒŒì¼ ì €ì¥
    cache_basic_path = f"./.cache/embeddings/{file.name}"
    cache_directory = os.path.dirname(cache_basic_path)
    cache_path = LocalFileStore(f"./.cache/embeddings/{file.name}")

    if not os.path.exists(cache_directory):
        os.makedirs(cache_directory, exist_ok=True)
        print(f"Cache Directory created: {cache_directory}")

    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )
    # íŒŒì¼ ë¡œë”
    loader = UnstructuredFileLoader(file_path)
    # ìª¼ê°  ë¬¸ì„œë“¤ì„ docs ë¦¬ìŠ¤íŠ¸ì— ë„£ì–´ë‘ . load_and_splitì€ List of docs ë°˜í™˜
    docs = loader.load_and_split(text_splitter=splitter)
    # OpenAIEmbeddings ë©”ì†Œë“œ ì‚¬ìš©
    embeddings = OpenAIEmbeddings()
    # ìºì‹œì— ì„ë² ë”©í•˜ì—¬ ì´ë¯¸ ìºì‹œë˜ì–´ ìˆëŠ” ê²½ìš° ì‘ì—…ì„ ìƒëµ. ì—†ìœ¼ë©´ ì„ë² ë”©.
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_path)
    # FAISS Vector Storeë¥¼ ì´ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ì €ì¥
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    # Vectorstoreì— ì €ì¥ëœ ê°’ì„ Retrieverë¡œ ë³€í™˜(ë¦¬íŠ¸ë¦¬ë²„ë¡œ ë³€í™˜í•´ì•¼ chainì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
    retriever = vectorstore.as_retriever()
    # retrieverì—ê²Œ invokeí•˜ëŠ” ê²ƒì€ ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ë¥¼ List of relavant docsë¡œ ë°˜í™˜
    # docs = retriever.invoke("ministry of truth")
    # st.write(docs) # í™•ì¸
    return retriever


# ë‹¤ë¥¸ ê³³ì—ì„œ ë˜ ì‚¬ìš©ë˜ë¯€ë¡œ ë³„ë„ì˜ í•¨ìˆ˜ë¡œ êµ¬ì„±
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})


# messageë¥¼ í™”ë©´ì— ë³´ì—¬ì£¼ê³ , session_stateì— ì €ì¥í•˜ì—¬ ë³´ê´€
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)


# session_stateì— ì €ì¥ëœ ë©”ì‹œì§€ë¥¼ í™”ë©´ì— ë³´ì—¬ì¤Œ, ì €ì¥ì€ ë¹„í™œì„±í™”
# ì €ì¥ì„ í•˜ëŠ” ê²½ìš° ì¤‘ë³µ ì €ì¥ë¨
def paint_history():
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False,
        )


# ì„ë² ë”©ëœ docsëŠ” ì—¬ëŸ¬ ê°œì˜ Documentë¥¼ ê°€ì§€ê³  ìˆìœ¼ë¯€ë¡œ ë‚´ìš©ë§Œ ë½‘ì•„ì„œ 2ì¤„ì”© ë„ì–´ ì¤Œìœ¼ë¡œì¨ AIê°€ ë” ì˜ êµ¬ë¶„í•  ìˆ˜ ìˆê²Œ í•¨
def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)


# í”„ë¡¬í”„íŠ¸ ì‘ì„±
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.

            Context: {context}
            """,
        ),
        ("human", "{question}"),
    ]
)


st.title("DocumentGPT")

st.markdown(
    """
Welcome!
            
Use this chatbot to ask questions to an AI about your files!

Upload your file on the sidebar.
"""
)

# íŒŒì¼ ì…ë ¥ ì°½ì„ ì‚¬ì´ë“œë°”ë¡œ ì´ë™
with st.sidebar:
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"],
    )

# íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¨ ê²½ìš° ì„ë² ë”©í•˜ê³ , ì„ë² ë”©ì´ ì™„ë£Œë˜ë©´ ì¤€ë¹„ì™„ë£Œ ë©”ì‹œì§€ë¥¼ ë³´ëƒ„
# aiê°€ ì¤€ë¹„ì™„ë£ŒëìŒì„ ë‚˜íƒ€ë‚´ëŠ” ë©”ì‹œì§€ëŠ” ì €ì¥í•  í•„ìš” ì—†ìŒ
if file:
    # embed_file ë©”ì†Œë“œë¥¼ í†µí•´ ë²¡í„°í™” ëœ ë¬¸ì„œë“¤ì„ ë°›ìŒ
    retriever = embed_file(file)
    send_message("I'm ready! Ask away!", "ai", save=False)
    # í™”ë©´ì— ì €ì¥ëœ ë©”ì‹œì§€ë¥¼ ë³´ì—¬ì¤Œ
    paint_history()
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë©”ì‹œì§€ë¥¼ ë³´ì—¬ì£¼ê³  ì €ì¥í•¨
    message = st.chat_input("Ask anything about your file...")
    if message:
        send_message(message, "human")
        # ì²´ì¸ ìƒì„±
        chain = (
            {
                # RunnableLamdaëŠ” í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ
                "context": retriever | RunnableLambda(format_docs),
                # ì§ˆë¬¸ì„ í†µê³¼ì‹œì¼œ ì „ë‹¬í•¨
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        # chain.invokeí•  ë•Œ ChatCallbackHandlerê°€ ì‘ë™í•¨
        # AIë¡œ í•˜ì—¬ê¸ˆ ë°•ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì—…ë°ì´íŠ¸í•˜ê²Œ í•˜ë ¤ë©´ chain.invokeë¥¼ st.chat_message ë‚´ë¶€ë¡œ ì˜®ê²¨ì£¼ê¸°ë§Œ í•˜ë©´ ë¨
        # ai chat_messageì—ì„œ invokeê°€ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— ai ë©”ì‹œì§€ ë‚´ë¶€ì— message_boxë¥¼ ìƒì„±í•˜ê²Œ ë¨
        # ì¦‰, chat_message ë¸”ë¡ ë‚´ì—ì„œ ì²´ì¸ì„ í˜¸ì¶œ
        with st.chat_message("ai"):
            chain.invoke(message)
else:
    # íŒŒì¼ì´ ì—†ê±°ë‚˜ ì—†ì–´ì§€ëŠ” ê²½ìš° session_stateë¥¼ ì´ˆê¸°í™”
    st.session_state["messages"] = []
