from langchain.document_loaders import SitemapLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# BeautifulSoup ì„í¬íŠ¸
from bs4 import BeautifulSoup
import streamlit as st
import requests

# import requests

st.set_page_config(
    page_title="SiteGPT",
    page_icon="ğŸ–¥ï¸",
)


# soupì€ beautiful soup objectì„. ê²€ìƒ‰, ì‚­ì œ ì‘ì—… ë“± ìˆ˜í–‰ ê°€ëŠ¥
# BeautifulSoupì„ ì„í¬íŠ¸ í›„ ëª…ì‹œì ìœ¼ë¡œ soupì— ì§€ì •í•´ì£¼ë©´ ìë™ì™„ì„±ë„ ì‚¬ìš© ê°€ëŠ¥
def parse_page(soup: BeautifulSoup):
    # documentì˜ ë‚´ìš© ì»¤ìŠ¤í„°ë§ˆì´ì§•
    header = soup.find("header")
    footer = soup.find("footer")
    if header:
        # .decomposeëŠ” í•˜ìœ„ íƒœê·¸ë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë‘ ì œê±°í•¨
        header.decompose()
    if footer:
        footer.decompose()
    # ì—¬ê¸°ì„œ ë°˜í™˜í•˜ëŠ” ê°’ì€ ë¬´ì—‡ì´ë“ ì§€ page_contentë¡œì¨ documentì— í¬í•¨ë˜ê²Œ ë¨
    return (
        str(soup.get_text())
        .replace("\n", " ")
        .replace("\xa0", " ")
        .replace("CloseSearch Submit Blog", "")
    )


@st.cache_data(show_spinner="Loading website...")
def load_website(url):
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=1000,
        chunk_overlap=200,
    )
    # URL ì»¤ìŠ¤í„°ë§ˆì´ì§•
    loader = SitemapLoader(
        url,
        filter_urls=[
            r"^(.*\/blog\/).*",
        ],
        # parsing_function propertyë¥¼ ì´ìš©í•˜ì—¬ íŒŒì‹±ì„ ìœ„í•œ í•¨ìˆ˜ë¥¼ ì‘ë™ì‹œí‚¬ ìˆ˜ ìˆìŒ
        parsing_function=parse_page,
    )
    loader.requests_per_second = 2
    docs = loader.load_and_split(text_splitter=splitter)
    return docs


st.markdown(
    """
    # SiteGPT
            
    Ask questions about the content of a website.
            
    Start by writing the URL of the website on the sidebar.
"""
)


with st.sidebar:
    url = st.text_input(
        "Write down a URL",
        placeholder="https://example.com",
    )


if url:
    # urlì— .xmlì´ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš° urlì„ ì¬ì…ë ¥í•˜ê²Œ í•¨
    if ".xml" not in url:
        with st.sidebar:
            st.error("Please write down a Sitemap URL.")
    else:
        # ìš”ì²­ ì‘ë‹µì´ xmlì¸ì§€ htmlì¸ì§€ í™•ì¸í•˜ëŠ”ë° ì‚¬ìš©
        # response = requests.get(url)
        # print(response.text)

        # urlë¡œë¶€í„° í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜´
        docs = load_website(url)
        st.write(docs)
